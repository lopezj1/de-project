{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To Do\n",
    "- Don't upload if already exists in bucket folder\n",
    "- Make unzip_blob a subflow?\n",
    "- Delete zip blob, once unzip complete\n",
    "- assign dtypes per file type - catch, size, trip\n",
    "- \n",
    "- Use pyspark or dbt to get into BQ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile, is_zipfile\n",
    "import pandas as pd\n",
    "from prefect import flow, task\n",
    "from google.cloud import storage\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task()\n",
    "def read_gcs_bucket(bucket) -> list:\n",
    "    \"\"\"Get list of zip folders in GCS bucket\"\"\"\n",
    "    lsblob = list(bucket.list_blobs(prefix=\"zip\"))  # get list of blobs in zip folder of bucket\n",
    "    lsblob = [l.name for l in lsblob]  # only return the filename from the blobs\n",
    "    print(lsblob)\n",
    "\n",
    "    return lsblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task()\n",
    "def convert_to_parquet(filename: str, file: str, bucket) -> pd.DataFrame:\n",
    "    \"\"\"Convert csv file to parquet file\n",
    "    (csv) -> parquet\"\"\"\n",
    "    tmp_dir = \"../tmp\"\n",
    "    Path(tmp_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    folder = filename.split(\"_\")[0]\n",
    "    trunc_fn = filename.split(\"_\")[1].split(\".\")[0]\n",
    "    output_file = f\"../tmp/{trunc_fn}\"\n",
    "    outfile = open(output_file, \"wb\")\n",
    "    outfile.write(file)\n",
    "    outfile.close()\n",
    "\n",
    "    if folder == \"catch\":\n",
    "        dtype = {\"user_id\": int, \"username\": \"string\"}\n",
    "        blob = bucket.blob(f\"catch/{trunc_fn}.parquet\")\n",
    "    elif folder == \"trip\":\n",
    "        dtype = {\"user_id\": int, \"username\": \"string\"}\n",
    "        blob = bucket.blob(f\"trip/{trunc_fn}.parquet\")\n",
    "    elif folder == \"size\":\n",
    "        dtype = {\"user_id\": int, \"username\": \"string\"}\n",
    "        blob = bucket.blob(f\"size/{trunc_fn}.parquet\")\n",
    "\n",
    "    df = pd.read_csv(output_file, dtype=dtype)\n",
    "    df.to_parquet(output_file.replace(\"csv\", \"parquet\"))\n",
    "\n",
    "    return trunc_fn, output_file, blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task()\n",
    "def write_gcs(filename: str, file, blob) -> None:\n",
    "    \"\"\"Write parquet file to GCS\n",
    "    (Dataframe) -> None\"\"\"\n",
    "    \n",
    "    with open(file, \"rb\") as myparquet:\n",
    "        blob.upload_from_file(myparquet)\n",
    "\n",
    "    print(f'Csv file size is {os.stat(f\"../tmp/{filename}.csv\").st_size}')\n",
    "    print(f'Parquet file size is {os.stat(f\"../tmp/{filename}.parquet\").st_size}')\n",
    "    os.remove(f\"../tmp/{filename}.csv\")  # delete/remove outfile\n",
    "    os.remove(f\"../tmp/{filename}.parquet\")  # delete/remove outfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def unzip_blob_csv(source_path: str, bucket) -> list:\n",
    "#     \"\"\"Unzip folder\"\"\"\n",
    "#     blob = bucket.blob(source_path)\n",
    "\n",
    "#     zipbytes = BytesIO(blob.download_as_string())\n",
    "\n",
    "#     if is_zipfile(zipbytes):\n",
    "#         with ZipFile(zipbytes, \"r\") as myzip:\n",
    "#             for contentfilename in myzip.namelist():\n",
    "#                 folder = contentfilename.split(\"_\")[0]\n",
    "#                 contentfile = myzip.read(contentfilename)\n",
    "\n",
    "#                 tmp_dir = \"../tmp\"\n",
    "#                 Path(tmp_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#                 trunc_fn = contentfilename.split(\"_\")[1]\n",
    "#                 output_file = f\"../tmp/{trunc_fn}\"\n",
    "#                 outfile = open(output_file, \"wb\")\n",
    "#                 outfile.write(contentfile)\n",
    "#                 outfile.close()\n",
    "\n",
    "#                 if folder == \"catch\":\n",
    "#                     blob = bucket.blob(f\"catch/{trunc_fn}\")\n",
    "#                 elif folder == \"trip\":\n",
    "#                     blob = bucket.blob(f\"trip/{trunc_fn}\")\n",
    "#                 elif folder == \"size\":\n",
    "#                     blob = bucket.blob(f\"size/{trunc_fn}\")\n",
    "\n",
    "#                 with open(output_file, \"rb\") as mycsv:\n",
    "#                     blob.upload_from_file(mycsv)\n",
    "\n",
    "#                 os.remove(f\"../tmp/{trunc_fn}\")  # delete/remove outfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flow()\n",
    "def unzip_blob(source_path: str, bucket) -> dict:\n",
    "    \"\"\"Unzip folder\"\"\"\n",
    "    blob = bucket.blob(source_path)\n",
    "\n",
    "    zipbytes = BytesIO(blob.download_as_string())\n",
    "\n",
    "    lsfilename = []\n",
    "    lsfile = []\n",
    "\n",
    "    if is_zipfile(zipbytes):\n",
    "        with ZipFile(zipbytes, \"r\") as myzip:\n",
    "            for contentfilename in myzip.namelist():\n",
    "                contentfile = myzip.read(contentfilename)\n",
    "\n",
    "                lsfilename.append(contentfilename)\n",
    "                lsfile.append(contentfile)\n",
    "    \n",
    "    dictfile = dict(zip(lsfilename, lsfile))\n",
    "    return dictfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flow(log_prints=False)\n",
    "def process_gcs_blob() -> None:\n",
    "    \"\"\"Process blob in gcs bucket\"\"\"\n",
    "    storage_client = storage.Client.from_service_account_json(\"../creds.json\")\n",
    "    bucket_name = \"de_project_bucket\"  # parameterize this\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    lsblob = read_gcs_bucket(bucket)\n",
    "\n",
    "    for blob in lsblob:\n",
    "        csvs = unzip_blob(blob, bucket)\n",
    "        for k, v in csvs.items():\n",
    "            print(k)\n",
    "            parquet = convert_to_parquet(k, v, bucket)\n",
    "            filename = parquet[0] #get file name without extension\n",
    "            file = parquet[1]\n",
    "            blob = parquet[2]\n",
    "            write_gcs(filename, file, blob)\n",
    "            # break\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    process_gcs_blob()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "254721729ad41773dee351bfff8b617bf657a0a6ef8ca8070296dad6f2976db6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
