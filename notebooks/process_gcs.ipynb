{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To Do\n",
    "- Don't upload if already exists in bucket folder\n",
    "- Make unzip_blob a subflow?\n",
    "- Convert to parquet before uploading to blob again?\n",
    "- Delete zip blob, once unzip complete\n",
    "- Unzip all folders\n",
    "- Check bucket size\n",
    "- Use pyspark or dbt to get into BQ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile, is_zipfile\n",
    "import pandas as pd\n",
    "from prefect import flow, task\n",
    "from google.cloud import storage\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @task()\n",
    "def read_gcs_bucket(bucket) -> list:\n",
    "    \"\"\"Get list of zip folders in GCS bucket\"\"\"\n",
    "    lsblob = list(\n",
    "        bucket.list_blobs(prefix=\"zip\")\n",
    "    )  # get list of blobs in zip folder of bucket\n",
    "    lsblob = [l.name for l in lsblob]  # only return the filename from the blobs\n",
    "    print(lsblob)\n",
    "\n",
    "    return lsblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @task()\n",
    "def unzip_blob(source_path: str, bucket) -> list:\n",
    "    \"\"\"Unzip folder\"\"\"\n",
    "    blob = bucket.blob(source_path)\n",
    "\n",
    "    zipbytes = BytesIO(blob.download_as_string())\n",
    "\n",
    "    if is_zipfile(zipbytes):\n",
    "        with ZipFile(zipbytes, \"r\") as myzip:\n",
    "            for contentfilename in myzip.namelist():\n",
    "                folder = contentfilename.split(\"_\")[0]\n",
    "                contentfile = myzip.read(contentfilename)\n",
    "\n",
    "                tmp_dir = \"../tmp\"\n",
    "                Path(tmp_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                trunc_fn = contentfilename.split(\"_\")[1]\n",
    "                output_file = f\"../tmp/{trunc_fn}\"\n",
    "                outfile = open(output_file, \"wb\")\n",
    "                outfile.write(contentfile)\n",
    "                outfile.close()\n",
    "\n",
    "                if folder == \"catch\":\n",
    "                    blob = bucket.blob(f\"catch/{trunc_fn}\")\n",
    "                elif folder == \"trip\":\n",
    "                    blob = bucket.blob(f\"trip/{trunc_fn}\")\n",
    "                elif folder == \"size\":\n",
    "                    blob = bucket.blob(f\"size/{trunc_fn}\")\n",
    "\n",
    "                with open(output_file, \"rb\") as mycsv:\n",
    "                    blob.upload_from_file(mycsv)\n",
    "\n",
    "                os.remove(f\"../tmp/{trunc_fn}\")  # delete/remove outfile\n",
    "\n",
    "    # tmp_dir = '../tmp'\n",
    "    # Path(tmp_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # with open('../tmp/archive.zip', 'wb') as file_obj:\n",
    "    #     blob.download_to_file(file_obj)\n",
    "\n",
    "    # zip_archive = ZipFile('../tmp/archive.zip', 'r')\n",
    "\n",
    "    # destination_blob = bucket.blob(destination_path)\n",
    "    # zip_archive.extractall(destination_path)\n",
    "\n",
    "    # return zip_archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @task()\n",
    "def convert_to_parquet():\n",
    "    \"\"\"Upload local parquet file to GCS\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @flow(log_prints=True)\n",
    "def process_gcs_blob() -> None:\n",
    "    \"\"\"Process blob in gcs bucket\"\"\"\n",
    "    storage_client = storage.Client.from_service_account_json(\"../creds.json\")\n",
    "    bucket_name = \"de_project_bucket\"  # parameterize this\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    lsblob = read_gcs_bucket(bucket)\n",
    "\n",
    "    source = lsblob[0]\n",
    "    lsfiles = unzip_blob(source, bucket)\n",
    "\n",
    "    # for blob in lsblob:\n",
    "    #     lsfiles = unzip_blob(blob, bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zip/ps_1981_1989_csv.zip', 'zip/ps_1990_1994_csv.zip', 'zip/ps_1995_1999_csv.zip', 'zip/ps_2000_2004_csv.zip', 'zip/ps_2005_2009_csv.zip', 'zip/ps_2010_2014_csv.zip', 'zip/ps_2015_2016_csv.zip', 'zip/ps_2017_csv.zip', 'zip/ps_2018_csv.zip', 'zip/ps_2019_csv.zip', 'zip/ps_2020_csv.zip', 'zip/ps_2021_csv.zip']\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == '__main__':\n",
    "process_gcs_blob()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "254721729ad41773dee351bfff8b617bf657a0a6ef8ca8070296dad6f2976db6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
